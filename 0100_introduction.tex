\section{Introduction}
Probability density estimation is a classical problem in statistics. Given independent observations $\mathbf X_1,\ldots,\mathbf X_n$ drawn from an unknown distribution on an interval $[a,b]\subset\R$, the goal is to construct an estimator $\hat f_n$ of the underlying density $f_0$. Accurate density estimates are useful both as a descriptive tool and as an intermediate step in more complex procedures, for example to generate synthetic samples or to study the modality of a distribution.

A possible approach is to assume that $f_0$ belongs to a parametric family $\{f_\vartheta:\vartheta\in\Theta\}$, where $\Theta$ is finite-dimensional. In this case the problem reduces to estimating $\vartheta$, for instance by maximum likelihood. When the family is well specified, parametric methods can be accurate and efficient. The disadvantage is that the choice of the family may result to be too rigid and miss features of the true density.

Another more general approach is to avoid fixing a finite-dimensional model for $f_0$ and to estimate the density in a nonparametric way. In this case, the estimated density $\hat f_n$ is selected from a flexible class of candidate densities rather than from a fixed parametric family. This has the advantage of allowing the estimator to capture features that are not well described by a simple parametric form. However, one disadvantage is that the properties of the estimated density are usually less immediate to characterize than in the parametric setting. We explore this second approach.

A possible strategy is to restrict the estimator to a finite-dimensional function space and to select the best element in that space by optimizing a suitable utility function. The dimension of the space is then allowed to grow with the sample size, so that the estimator becomes more flexible as more data are available.

In this thesis we focus on spline-based estimators. Splines are a structured but flexible class of functions, with a large literature and well-established approximation properties. They admit simple finite-dimensional representations, which makes them convenient to use in algorithms and be evaluated efficiently on a computer. However, increasing flexibility typically increases the variance of the estimator, producing noisy fitted densities. For this reason, we consider utilities that include a regularization term that helps control overfitting.

In \autoref{ch:chapter1}, we present the general statistical framework developed by Huang and Su in \cite{huang2021} for penalized spline estimation. Given an iid sample and a target function that depends on its distribution, the framework defines an estimator by maximizing a penalized empirical utility over a sequence of spline spaces. Under suitable hypotheses, Huang and Su derive probabilistic bounds for both the estimation error and the penalty term. In addition, we present a range of propositions and lemmas that make these assumptions easier to verify in practice.

In \autoref{ch:chapter2}, we present five methods for probability density estimation. For each method, we derive a the associated utility function and we verify whether the conditions required by the framework in \autoref{ch:chapter1} are satisfied. The first two methods follow a maximum likelihood approach. The remaining three methods are based on score matching, introduced by Hyv\"arinen in \cite{score_matching}, which replaces likelihood maximization with the estimation of the score function and has the advantage of avoiding the numerical computation of normalizing constants.

In \autoref{ch:chapter3}, we describe how the presented methods can be implemented on a computer. We explain how splines are represented in a finite basis, how the penalty term is computed in matrix form, and how the resulting optimization problems are solved using the Newton-Raphson method. In addition, we present how the parameter that controls strength of the penalty can be chosen by validation or cross-validation, and we discuss computational complexity of the algorithms.

Finally, in \autoref{ch:chapter4} we report numerical experiments on both synthetic and real-world data. On synthetic examples, where the ground-truth density is known, we compare the methods and study the effect of the sample size, the number of knots, and the penalty parameter. Then, we consider real datasets to illustrate the behavior of the estimators in practice. We include a study of the execution times to compare theoretical and empirical runtime complexity.