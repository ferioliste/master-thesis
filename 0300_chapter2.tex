\section{Methods for density estimation}\label{ch:chapter2}
In this chapter, we present and analyze several methods for probability density estimation.

We consider a continuous random variable $\mathbf X$, that takes values in $[a,b]$, with unknown density function $f_0\in W^p$. Assume we have available $n$ iid copies of $\mathbf X$, denoted as $\mathbf X_1,\dots,\mathbf X_n$. We are interested in finding an estimate $f$ of $f_0$ using this sample.

Let $\mu$ be the Lebesgue measure. We recall that, in order to be a density, $f$ needs to satisfy the following two properties:
\begin{enumerate}
    \item $f \geq 0$ $\mu$-a.e. on $[a,b]$.
    \item $\int_a^b f(x)\de x=1$
\end{enumerate}
Note that, since $f$ is continuous, the first condition is equivalent to $f(x) \geq 0$ for all $x\in [a,b]$.

In the following sections, we will derive and analyze several utility functions for probability density estimation. In order to define the utility functions, we will first define the expected utility and then we will obtain the utility by replacing expectations with averages.

In order to verify that the presented utility functions are valid we use the following sufficient condition.

\begin{proposition}[Sufficient condition for validity of utility functions]\label{prop:validity}
Let $\{\varphi_0,\ldots,\varphi_{m+k}\}$ be a basis of the spline space $\mathbb G$. For any spline $g\in\mathbb G$ there exists a unique coefficient vector $\theta\in\R^{m+k+1}$ such that
\begin{equation*}
    g(x) = \theta^\tr \varphi(x) = \sum_{i=0}^{m+k} \theta_i\varphi_i(x),
\end{equation*}
where $\varphi(x)\in\R^{m+k+1}$ is such that $\big(\varphi(x)\big)_i = \varphi_i(x)$ for $i=0,\ldots,m+k$.

We define
\begin{equation*}
    \tilde\ell(\theta;\mathbf W_1,\ldots,\mathbf W_n) = \ell\big(\theta^\tr \varphi(x);\mathbf W_1,\ldots,\mathbf W_n\big)\qquad\text{and}\qquad\tilde\Lambda(\theta) = \Lambda\big(\theta^\tr \varphi(x)\big)
\end{equation*}
for $\theta\in\R^{m+k+1}$.

Assume the following conditions hold.
\begin{enumerate}[label=\roman*)]
    \item $\displaystyle \min_{h\in W^p} \Lambda(h) = \Lambda(\eta_0)$;
    \item The Hessian $\nabla^2_{\theta} \tilde\ell(\theta;\mathbf W_1,\ldots,\mathbf W_n)$ of $\tilde\ell$ is well-defined and negative semi-definite for all $\theta\in\R^{m+k+1}$ and for all possible values of $\mathbf W_1,\ldots,\mathbf W_n$;
    \item The Hessian $\nabla^2_{\theta} \tilde\Lambda(\theta)$ of $\tilde\Lambda$ is well-defined and strictly negative definite for all $\theta\in\R^{m+k+1}$.
\end{enumerate}

Then $\ell$ is a valid utility function.
\end{proposition}

\begin{proof}\mbox{}

We verify the three properties of a valid utility function. The first property is automatically satisfied.

Let $h_1,h_2\in\mathbb G$ and $0\le\alpha\le1$. Let $\theta_1,\theta_2\in\mathbb R^{m+k+1}$ the coefficient vectors of $h_1$ and $h_2$, respectively. Since $\mathbb G$ is a linear space,
\begin{equation*}
    \alpha h_1 + (1-\alpha)h_2
= (\alpha\theta_1 + (1-\alpha)\theta_2)^\tr \varphi(x).
\end{equation*}
For the second property, we have
\begin{equation*}
    \ell(\alpha h_1 + (1-\alpha)h_2;\mathbf W_1,\ldots,\mathbf W_n)
= \tilde\ell(\alpha\theta_1 + (1-\alpha)\theta_2;
\mathbf W_1,\ldots,\mathbf W_n).
\end{equation*}
By assumption (ii), the Hessian
$\nabla^2_{\theta}\tilde\ell(\theta;\mathbf W_1,\ldots,\mathbf W_n)$
is negative semi-definite for all $\theta$ and all possible values of $\mathbf W_1,\ldots,\mathbf W_n$.
Therefore, $\tilde\ell$ is concave on $\mathbb R^{m+k+1}$, which implies
\begin{equation*}
    \tilde\ell(\alpha\theta_1 + (1-\alpha)\theta_2;\mathbf W_1,\ldots,\mathbf W_n)
    \geq
    \alpha \tilde\ell(\theta_1;\mathbf W_1,\ldots,\mathbf W_n) + (1-\alpha)\tilde\ell(\theta_2;\mathbf W_1,\ldots,\mathbf W_n)
\end{equation*}
that is the same as
\begin{equation*}
    \ell(\alpha h_1 + (1-\alpha)h_2;\mathbf W_1,\ldots,\mathbf W_n)
\geq
\alpha \ell(h_1;\mathbf W_1,\ldots,\mathbf W_n)
+ (1-\alpha)\ell(h_2;\mathbf W_1,\ldots,\mathbf W_n).
\end{equation*}

For the third property, we have
\begin{equation*}
    \Lambda(\alpha h_1 + (1-\alpha)h_2)
= \tilde\ell(\alpha\theta_1 + (1-\alpha)\theta_2).
\end{equation*}
By assumption (iii), the Hessian
$\nabla^2_{\theta}\tilde\Lambda(\theta)$
is strictly negative definite for all $\theta$.
Therefore, $\tilde\Lambda$ is strictly concave on $\mathbb R^{m+k+1}$, which implies that, for $\theta_1\neq\theta_2$,
\begin{equation*}
    \tilde\Lambda(\alpha\theta_1 + (1-\alpha)\theta_2)
    >
    \alpha \tilde\Lambda(\theta_1) + (1-\alpha)\tilde\Lambda(\theta_2)
\end{equation*}
that is the same as
\begin{equation*}
    \Lambda(\alpha h_1 + (1-\alpha)h_2)
>
\alpha \Lambda(h_1)
+ (1-\alpha)\Lambda(h_2),
\end{equation*}
for $h_1\neq h_2$.
\end{proof}

We also present and prove a lemma that will be useful later to verify the hypotheses of \autoref{prop:validity}.
\begin{lemma}\label{lem:support}
    We consider a continuous random variable $\mathbf X$ that takes values in $[a,b]$. Let $g\in\mathbb G$ such that $g(\mathbf X) = c$ almost surely for some constant $c\in\R$. If
    \begin{equation*}
        \Pr(\mathbf X \in [t_i, t_{i+1}]) > 0\qquad\text{for all}\ \ i=0,\ldots,k,
    \end{equation*}
    then $g(x) = c$ for all $x\in[a,b]$.

    In particular, this is true if $\mathrm{supp}(\mathbf X) = [a,b]$, where $\mathrm{supp}(\mathbf X)$ is the support of $\mathbf X$.
\end{lemma}

\begin{proof}\mbox{}

Define $\tilde g(x) := g(x)-c$, then $\tilde g\in\mathbb G$ and $\tilde g(\mathbf X) = 0$ almost surely.

For $i\in\{0,\ldots,k\}$, using the hypothesis, we have
\begin{equation*}
    \Pr(\mathbf X\in[t_i,t_{i+1}],\, \tilde g(\mathbf X) = 0) = \Pr(\mathbf X\in[t_i,t_{i+1}]) > 0.
\end{equation*}
Then, since $\mathbf X$ is continuous, the set
\begin{equation*}
    A_i = \{x\in[t_i,t_{i+1}]: \tilde g(\mathbf X)=0\}
\end{equation*}
has positive Lebesgue measure. Since $\tilde g$ is a polynomial on $[t_i,t_{i+1}]$ we necessarily have $\tilde g(x)=0$ for all $x\in[t_i,t_{i+1}]$. Repeating the same argument for all $i\in\{0,\ldots,k\}$, it follows that $\tilde g(x)=0$ and $g(x) = c$ for all $x\in[a,b]$.

Finally, if $\mathrm{supp}(\mathbf X) = [a,b]$, then $\Pr(\mathbf X \in [t_i, t_{i+1}]) > 0$ for all $i=0,\ldots,k$, so the above argument applies.
\end{proof}

\input{0301_simple_MLE}
\input{0302_shifted_MLE}
\input{0303_simple_score_matching}
\input{0304_shifted_score_matching}
\input{0305_generalized_score_matching}