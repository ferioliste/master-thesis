\subsection{Shifted Maximum Likelihood}\label{sec:shifted_MLE}
In this section, we present a modified version of the classical maximum likelihood estimator of the density. This will allow us to remove the assumption that $f_0$ is bounded away from $0$, at the cost of having a (slightly) biased estimator.

For $0<\gamma<1$, we consider the random variable $\widetilde{\mathbf X}$ with density
\begin{equation*}
    \tilde f_0(\cdot) = (1-\gamma)f_0(\cdot) + \frac{\gamma}{b-a}.
\end{equation*}
Equivalently, $\widetilde{\mathbf X}$ is distributed as
\begin{equation*}
    \widetilde{\mathbf X} \sim 
    \begin{cases}
        \mathbf X, & \text{with probability } 1-\gamma,\\[0.2em]
        \mathbf U, & \text{with probability } \gamma,
    \end{cases}
\end{equation*}
where $\mathbf X$ has density $f_0$ on $[a,b]$ and $\mathbf U$ is independent of $\mathbf X$ and uniformly distributed on $[a,b]$. We will construct an estimator of $\tilde f_0$.

As for Simple Maximum Likelihood, we represent $f \in W_p\cap\mathcal D$ as
\begin{equation*}
    f(\cdot) = \exp(\eta(\cdot))\Big/ \int_a^b \exp \eta(x)\de x,
\end{equation*}
with $\eta\in W_p$ and
\begin{equation}\label{eq:cond_eta_shifted_MLE}
    \int_a^b\eta(x)\de x = 0.
\end{equation}

Then, we define $\Lambda$ as
\begin{gather}
    \Lambda(\eta) = \E\big[\ln\!\big(f(\widetilde{\mathbf X})\big)\big] = \E\big[\eta(\widetilde{\mathbf X})\big] - \ln \int_a^b \exp \eta(x)\de x \notag\\
    = (1-\gamma)\E\big[\eta(\mathbf X)\big] + \gamma\E\big[\eta(\mathbf U)\big] - \ln \int_a^b \exp \eta(x)\de x \notag \\
    = (1-\gamma)\E\big[\eta(\mathbf X)\big] + \frac{\gamma}{b-a}\int_a^b\eta(x)\de x - \ln \int_a^b \exp \eta(x)\de x, \label{eq:Lambda_shifted_MLE}
\end{gather}
so that $\ell$ can be defined as
\begin{gather}
    \ell(\eta; \mathbf X_1 , \ldots, \mathbf X_n) = \frac{1}{n}\sum_{i=1}^n l(\eta; \mathbf X_i) \notag \\
    = \frac{1}{n}\sum_{i=1}^n \bigg((1-\gamma)\eta(\mathbf X_i) + \frac{\gamma}{b-a}\int_a^b\eta(x)\de x - \ln \int_a^b \exp \eta(x)\de x\bigg). \label{eq:ell_shifted_MLE}
\end{gather}

In this case, we do not need to assume $f_0$ is bounded away from $0$. However, we still assume that $f_0$ is bounded away from infinity. This means that there exists $C_2^*$ such that
\begin{equation}\label{eq:f_bounded_shifted_MLE}
    f_0\leq C_2^*\ \ \mu\text{-a.e.}\ .
\end{equation}

First, we show that $\ell$ is a valid utility function. Then, we verify that $\Lambda$, $\ell$ and $l$ satisfy the hypotheses of Lemmas $\ref{lem:cond_1}$, \ref{lem:cond_3}, and \ref{lem:cond_2}, so that we are able to apply Theorems \ref{thm:approx_error} and \ref{thm:est_error}.

\begin{proposition}
    The function $\ell$ as in \autoref{eq:ell_shifted_MLE} is a valid utility function.
\end{proposition}

\begin{proof}\mbox{}

We verify the hypotheses of \autoref{prop:validity} so we can conclude that $\ell$ is valid.

Condition (i) is satisfied by construction.

As in the proof of \autoref{prop:MLE_valid}, we consider the restricted spline space 
\begin{equation*}
    \mathbb G^\times = \big\{\eta\in\mathbb G:{\textstyle\int_a^b\eta(x)\de x = 0}\big\}.
\end{equation*}

We define $\tilde\ell$ and $\tilde\Lambda$ as in \autoref{prop:validity}. We have
\begin{gather*}
    \tilde\ell(\theta; \mathbf X_1 , \ldots, \mathbf X_n) \\
    = \frac{1}{n}\sum_{v=1}^n \bigg((1-\gamma)\theta^\tr \tilde\varphi(\mathbf X_v) + \frac{\gamma}{b-a}\int_a^b\theta^\tr \tilde\varphi(x)\de x - \ln \int_a^b \exp\big(\theta^\tr \tilde\varphi(x)\big)\de x\bigg)
\end{gather*}
and
\begin{equation*}
    \Lambda(\theta) = (1-\gamma)\E\big[\theta^\tr \tilde\varphi(\mathbf X)\big] + \frac{\gamma}{b-a}\int_a^b\theta^\tr \tilde\varphi(x)\de x - \ln \int_a^b \exp\big(\theta^\tr \tilde\varphi(x)\big)\de x,
\end{equation*}
for $\theta\in\R^{m+k}$ and $\{\tilde\varphi_1,\ldots, \tilde\varphi_{m+k}\}$ basis of $\mathbb G^\times$.

Differentiating twice, we find
\begin{gather*}
    \big(\H(\theta)\big)_{i,j} := \big(\nabla^2_{\theta} \tilde\ell(\theta;\mathbf W_1,\ldots,\mathbf W_n)\big)_{i,j} = \big(\nabla^2_{\theta} \tilde\Lambda(\theta)\big)_{i,j} \\
    = \frac{\int_a^b \tilde\varphi_i(x)\exp\!\big(\theta^\tr \tilde\varphi(x)\big)\de x \int_a^b \tilde\varphi_j(x)\exp\!\big(\theta^\tr \tilde\varphi(x)\big)\de x}{\Big(\int_a^b\exp\!\big(\theta^\tr \tilde\varphi(x)\big)\de x\Big)^2}
    - \frac{\int_a^b \tilde\varphi_i(x)\tilde\varphi_j(x)\exp\!\big(\theta^\tr \tilde\varphi(x)\big)\de x}{\Big(\int_a^b\exp\!\big(\theta^\tr \tilde\varphi(x)\big)\de x\Big)^2}.
\end{gather*}

We notice that $\H(\theta)$ is the exactly the same as in the proof of \autoref{prop:MLE_valid}, so the rest of the proof is identical.
\end{proof}

\begin{proposition}\label{prop:shifted_MLE_cond_1}
    The functional $\Lambda$ as in \autoref{eq:Lambda_shifted_MLE} satisfies the hypothesis of \autoref{lem:cond_1}, and so condition (1) of \autoref{thm:approx_error} is satisfied.
\end{proposition}

\begin{proof}\mbox{}

In order to prove this proposition, we only need to notice that $\Lambda$ in \autoref{eq:Lambda_shifted_MLE} is exactly what $\Lambda$ in \autoref{eq:Lambda_simple_MLE} would be if we replaced $f_0$ with $\tilde f_0$. In addition, we have that $\tilde f_0$ is bounded away from $0$ and infinity. Then, we can use \autoref{prop:simple_MLE_cond_1} and conclude.
\end{proof}

\begin{proposition}
    The function $l$ as in \autoref{eq:ell_shifted_MLE} satisfies the hypothesis of \autoref{lem:cond_2}, and so condition (1) of \autoref{thm:est_error} is satisfied.
\end{proposition}

\begin{proof}\mbox{}

We recall that
\begin{equation*}
    \dot l[\bar\eta_n; g] = \frac{\de}{\de\alpha}l(\bar\eta_n+\alpha g)\bigg|_{\alpha=0^+}.
\end{equation*}

In our case,
\begin{equation*}
    l(\eta, \mathbf X) = (1-\gamma)\eta(\mathbf X) + \frac{\gamma}{b-a}\int_a^b\eta(x)\de x - \ln \int_a^b \exp \eta(x)\de x.
\end{equation*}
Then,
\begin{gather*}
    l(\bar\eta_n+\alpha g) = (1-\gamma)\big(\bar\eta_n(\mathbf X) +\alpha g(\mathbf X)\big) \\
    + \frac{\gamma}{b-a}\int_a^b(\bar\eta_n(x)+\alpha g(x))\de x - \ln\int_a^b\exp(\bar\eta_n(x)+\alpha g(x))\de x, \\
    \frac{\de}{\de\alpha}l(\bar\eta_n+\alpha g) = (1-\gamma)g(\mathbf X) + \frac{\gamma}{b-a}\int_a^b g(x)\de x - \frac{\int_a^bg(x)\exp(\bar\eta_n(x)+\alpha g(x))\de x}{\int_a^b\exp(\bar\eta_n(x)+\alpha g(x))\de x}, \text{ and}\\
    \dot l[\bar\eta_n; g]=(1-\gamma)g(\mathbf X) + \frac{\gamma}{b-a}\int_a^b g(x)\de x - \frac{\int_a^bg(x)\exp(\bar\eta_n(x))\de x}{\int_a^b\exp(\bar\eta_n(x))\de x}.
\end{gather*}
It follows that
\begin{gather*}
    \var\big(\dot l[\bar\eta_n; g]\big) = \var\big((1-\gamma)g(\mathbf X)\big) \leq \var\big(g(\mathbf X)\big) \\
    = \E[g(\mathbf X)^2] - \E[g(\mathbf X)]^2 \leq \E[g(\mathbf X)^2] \leq C_2^*\|g\|_2^2,
\end{gather*}
since $f_0$ is bounded away from infinity.

Since the hypothesis is satisfied, we can use \autoref{lem:cond_2} and conclude that in this setting condition (1) of \autoref{thm:est_error} is satisfied.
\end{proof}

\begin{proposition}
    The function $\ell$ as in \autoref{eq:ell_shifted_MLE} satisfies the hypotheses of \autoref{lem:cond_3}, and so condition (2) of \autoref{thm:est_error} is satisfied.
\end{proposition}

\begin{proof}\mbox{}

Since condition (1) of \autoref{thm:approx_error} is satisfied we can apply \autoref{thm:approx_error} and obtain that $\|\bar\eta_n\|_\infty$ is bounded and so condition (i) of \autoref{lem:cond_3} is satisfied.

Following a similar procedure as in the proof of \autoref{prop:simple_MLE_cond_3}, we can show that
\begin{equation*}
    \frac{\de^2}{\de\alpha^2}\ell(\bar\eta_n + \alpha g) = \frac{\de^2}{\de\alpha^2}\Lambda(\bar\eta_n + \alpha g).
\end{equation*}
It follows that $\ell(\bar\eta_n + \alpha g)$ is twice differentiable.

In addition, since, by \autoref{prop:simple_MLE_cond_1}, $\Lambda$ satisfies the hypothesis of \autoref{lem:cond_1}, we have
\begin{equation*}
    \frac{\de^2}{\de\alpha^2}\ell(\bar\eta_n + \alpha g) = \frac{\de^2}{\de\alpha^2}\Lambda(\bar\eta_n + \alpha g) \leq -M_2 \|g\|_2^2,\qquad\text{ for }0\leq\alpha\leq1.
\end{equation*}

Since the hypotheses are satisfied, we can use \autoref{lem:cond_3} and conclude that in this setting condition (2) of \autoref{thm:est_error} is satisfied.
\end{proof}

We conclude this section by showing how the estimator can be made asymptotically unbiased. We consider the penalized spline estimator
\begin{equation*}
    \hat\eta_n = \argmax_{h\in \mathbb G} \p\ell(h; \mathbf{W}_1 , \ldots, \mathbf{W}_n),
\end{equation*}
so that the estimated density of $\mathbf X$ will be
\begin{equation*}
    f_1(x) = \exp(\hat\eta_n(x))\Big/ \int_a^b \exp \hat\eta_n(y)\de y,\qquad\text{for }x\in[a,b].
\end{equation*}

A natural way to make $f_1$ unbiased is to define
\begin{equation*}
    f_2(x) = \frac{f_1(x)}{1-\gamma}-\frac{1}{b-a}\frac{\gamma}{1-\gamma},\qquad\text{for }x\in[a,b].
\end{equation*}
However, this does not guarantee that $f_2(x)\geq 0$ for all $x\in[a,b]$. To solve this issue, one might be tempted to define
\begin{equation*}
    f_3(x) = 0 \vee f_2(x)\Big/ \int_a^b \big(0 \vee f_2(y)\big)\de y,\qquad\text{for }x\in[a,b].
\end{equation*}
Unfortunately, this ruins completely the regularity of $f_2$ since it introduces corner points where the first derivative is discontinuous. Instead, we define
\begin{equation*}
    f_4(x) = f_2(x) + 0 \wedge\min_{y\in[a,b]} f_2(y) \Big/ \Big(1+(b-a)\big(0 \wedge\min_{y\in[a,b]} f_2(y)\big)\Big),
\end{equation*}
where the minimum of $f_2$ over $[a,b]$ always exists since $f_2$ is continuous. Intuitively, the estimator $f_4$ is $f_2$ shifted up by the bare minimum to make $f_2$ positive for all $x\in[a,b]$. In addition, by conclusion (a) of \autoref{thm:overall_error}, we have that $f_4$ is an asymptotically unbiased estimator.