\subsection{Shifted Score Matching}
In this section, we present a modified version of the simple score matching estimator of the density. This will allow us to satisfy the hypothesis of \autoref{thm:approx_error}, at the cost of having a (slightly) biased estimator.

As for the simple score matching estimator, we assume $f_0(a) = f_0(b) = 0$. In addition, we assume that $f_0$ is bounded away from infinity. This means that there exists $C_2^*$ such that
\begin{equation}
    f_0\leq C_2^*\ \ \mu\text{-a.e.}\ .
\end{equation}

Similarly to \autoref{sec:shifted_MLE}, for $0<\gamma<1$, we consider the random variable $\widetilde{\mathbf X}$ with density
\begin{equation*}
    \tilde f_0(\cdot) = (1-\gamma)f_0(\cdot) + \frac{\gamma}{b-a}.
\end{equation*}
Equivalently, $\widetilde{\mathbf X}$ is distributed as
\begin{equation*}
    \widetilde{\mathbf X} \sim 
    \begin{cases}
        \mathbf X, & \text{with probability } 1-\gamma,\\[0.2em]
        \mathbf U, & \text{with probability } \gamma,
    \end{cases}
\end{equation*}
where $\mathbf X$ has density $f_0$ on $[a,b]$ and $\mathbf U$ is independent of $\mathbf X$ and uniformly distributed on $[a,b]$. We will construct an estimator of $\tilde f_0$.

We define the score $\widetilde\psi_0$ of $\tilde f_0$ as
\begin{equation*}
    \widetilde\psi_0(x) = \frac{\de}{\de x}\ln \tilde f_0(x) = \frac{\de}{\de x}\ln\Big((1-\gamma)f_0(\cdot) + \frac{\gamma}{b-a}\Big), \qquad\text{with }x\in[a,b],
\end{equation*}
where $\widetilde\psi_0\in W^p$.

As in the simple score matching method, we want to estimate $\widetilde\psi_0$ and then use \autoref{eq:psi_to_f} to obtain an estimate of $\tilde f_0$.

Let $\psi\in W^p$. We want to minimize the expected squared distance between $\psi(\widetilde{\mathbf X})$ and $\psi_0(\widetilde{\mathbf X})$, that is
\begin{gather*}
    J(\psi) = \frac{1}{2} \E \Big[(\psi(\widetilde{\mathbf X}) - \widetilde\psi_0(\widetilde{\mathbf X}))^2\Big] \\
    = \frac{1}{2}\int_a^b \psi(x)^2 \tilde f_0(x)\de x - \int_a^b \psi(x)\widetilde\psi_0(x)\tilde f_0(x)\de x + \frac{1}{2}\int_a^b \widetilde\psi_0(x)^2\tilde f_0(x)\de x.
\end{gather*}
As in the simple score matching method, we can remove the third term as it does not depend on $\psi$. The second term can be rewritten as
\begin{gather*}
    \int_a^b \psi(x)\widetilde\psi_0(x)\tilde f_0(x)\de x = \int_a^b \psi(x)\tilde f_0'(x)\de x = (1-\gamma)\int_a^b \psi(x)f_0'(x)\de x = \\
    (1-\gamma)\big[\psi(x)f_0(x)\big]_a^b - (1-\gamma)\int_a^b \psi'(x)f_0(x)\de x = - (1-\gamma)\int_a^b \psi'(x)f_0(x)\de x
\end{gather*}
where the first equality uses the fact that
\begin{equation*}
    \tilde f_0(x)\widetilde\psi_0(x) = \tilde f_0(x)\frac{\de}{\de x}\ln \tilde f_0(x) = \tilde f_0(x)\frac{\tilde f_0'(x)}{\tilde f_0(x)} = \tilde f_0'(x),
\end{equation*}
the second equality is by integration by parts, and the third equality uses the fact that $f_0(a) = f_0(b) = 0$. The first term can be rewritten as
\begin{gather*}
    \frac{1}{2}\int_a^b \psi(x)^2 \tilde f_0(x)\de x = \frac{1}{2}\int_a^b \psi(x)^2 \bigg((1-\gamma)f_0(x) + \frac{\gamma}{b-a}\bigg)\de x \\
    = \frac{1}{2}(1-\gamma)\int_a^b \psi(x)^2 f_0(x)\de x + \frac{1}{2} \frac{\gamma}{b-a}\int_a^b \psi(x)^2 \de x
\end{gather*}

As before, we multiply $J(\psi)$ by $-1$ in order to turn a loss function into a utility function.

Then, we define $\Lambda$ as
\begin{gather}
    \Lambda(\psi) = - (1-\gamma)\int_a^b \psi'(x)f_0(x)\de x -\frac{1}{2}(1-\gamma)\int_a^b \psi(x)^2 f_0(x)\de x - \frac{1}{2}\frac{\gamma}{b-a}\int_a^b \psi(x)^2 \de x \notag \\
    = \E\bigg[\!- (1-\gamma)\psi'(\mathbf X) -\frac{1}{2}(1-\gamma)\psi(\mathbf X)^2\bigg] - \frac{1}{2}\frac{\gamma}{b-a}\int_a^b \psi(x)^2 \de x, \label{eq:Lambda_shifted_SM}
\end{gather}
so that $\ell$ can be defined as
\begin{gather}
    \ell(\psi; \mathbf X_1 , \ldots, \mathbf X_n) = \frac{1}{n}\sum_{i=1}^n l(\psi; \mathbf X_i) \notag \\
    = \frac{1}{n}\sum_{i=1}^n \bigg(\!- (1-\gamma)\psi'(\mathbf X_i) - \frac{1}{2}(1-\gamma)\psi(\mathbf X_i)^2 - \frac{1}{2}\frac{\gamma}{b-a}\int_a^b \psi(x)^2 \de x\bigg). \label{eq:ell_shifted_SM}
\end{gather}

We notice that, in practice, the integral $\int_a^b \psi(x)^2\de x$ does not need to be approximated numerically. Indeed, for $\psi\in\mathbb G$, $\psi^2$ is a polynomial on each interval $[t_i,t_{i+1}]$. Therefore, the integral can be computed exactly as
\begin{equation*}
    \int_a^b \psi(x)^2\de x = \sum_{i=1}^k \int_{t_i}^{t_{i+1}} \psi(x)^2\de x.
\end{equation*}

We will show that $\Lambda$ satisfies the hypothesis of \autoref{lem:cond_1}, so that we are able to apply \autoref{thm:approx_error}.

\begin{proposition}
The functional $\Lambda$ as in \autoref{eq:Lambda_shifted_SM} satisfies the hypothesis of \autoref{lem:cond_1}, and so Condition 1 of \autoref{thm:approx_error} is satisfied.
\end{proposition}

\begin{proof}\mbox{}

We have that
\begin{gather*}
    \Lambda(h_1 + \alpha h_2) = \E\bigg[\!- (1-\gamma)\big(h_1'(\mathbf X) + \alpha h_2'(\mathbf X)\big) -\frac{1}{2}(1-\gamma)\big(h_1(\mathbf X)+ \alpha h_2(\mathbf X)\big)^2\bigg] \\
    - \frac{1}{2}\frac{\gamma}{b-a}\int_a^b \big(h_1(x)+ \alpha h_2(x)\big)^2 \de x, \\
    \frac{\de}{\de\alpha}\Lambda(h_1 + \alpha h_2) = \E\big[\!- (1-\gamma)h_2'(\mathbf X) -(1-\gamma)h_2(\mathbf X)\big(h_1(\mathbf X)+ \alpha h_2(\mathbf X)\big)\big] \\
    - \frac{\gamma}{b-a}\int_a^b h_2(x)\big(h_1(x)+ \alpha h_2(x)\big) \de x,\text{ and} \\
    \frac{\de^2}{\de\alpha^2}\Lambda(h_1 + \alpha h_2)
    = \E\big[\!-(1-\gamma)h_2(\mathbf X)^2\big]
    - \frac{\gamma}{b-a}\int_a^b h_2(x)^2 \de x.
\end{gather*}

Since $f_0$ is bounded away from infinity,
\begin{equation*}
    0\leq\E\big[h_2(\mathbf X)^2\big]\leq C_2^*\|h_2\|_2^2.
\end{equation*}
Then,
\begin{equation*}
    -\Big((1-\gamma)C_2^* + \frac{\gamma}{b-a}\Big)\|h_2\|_2^2 \leq \Lambda(h_1 + \alpha h_2) \leq -\frac{\gamma}{b-a}\|h_2\|_2^2
\end{equation*}

Since the hypotheses are satisfied, we can use \autoref{lem:cond_1} and conclude that in this setting Condition 2 of \autoref{thm:approx_error} is satisfied.
\end{proof}

Unfortunately, to our knowledge, $l$ still does not satisfy the hypothesis of \autoref{thm:est_error}. We report here our attempt to prove the hypothesis of \autoref{lem:cond_2_bis} and explain why it fails.

We recall that
\begin{equation*}
    \dot l[\bar\eta_n; g] = \frac{\de}{\de\alpha}l(\bar\eta_n+\alpha g)\bigg|_{\alpha=0^+}.
\end{equation*}

For $h\in\mathbb G$ with $\|h\|_2\leq 1$, we have
\begin{gather*}
    l(\bar\eta_n+\alpha g) = - (1-\gamma)\big(\bar\eta_n'(\mathbf X)+\alpha g'(\mathbf X)\big) - \frac{1}{2}(1-\gamma)\big(\bar\eta_n(\mathbf X)+\alpha g(\mathbf X)\big)^2 \\
    - \frac{1}{2}\frac{\gamma}{b-a}\int_a^b \big(\bar\eta_n(x)+\alpha g(x)\big)^2 \de x,\\
    \frac{\de}{\de\alpha}l(\bar\eta_n+\alpha g) = - (1-\gamma)g'(\mathbf X) - \frac{1}{2}(1-\gamma)g(\mathbf X)\big(\bar\eta_n(\mathbf X)+\alpha g(\mathbf X)\big) \\
    - \frac{1}{2}\frac{\gamma}{b-a}\int_a^b g(x)\big(\bar\eta_n(x)+\alpha g(x)\big) \de x, \text{ and}\\
    \dot l[\bar\eta_n; g] = - (1-\gamma)g'(\mathbf X) - \frac{1}{2}(1-\gamma)g(\mathbf X)\bar\eta_n(\mathbf X) - \frac{1}{2}\frac{\gamma}{b-a}\int_a^b g(x)\bar\eta_n(x)\de x.
\end{gather*}
It follows that
\begin{gather*}
    \var\big(\dot l[\bar\eta_n; g]\big) \leq \E\Big[\Big(- (1-\gamma)g'(\mathbf X) - \frac{1}{2}(1-\gamma)g(\mathbf X)\bar\eta_n(\mathbf X)\Big)^2\Big] \\
    \leq \E\Big[\Big(g'(\mathbf X) + \frac{1}{2}g(\mathbf X)\bar\eta_n(\mathbf X)\Big)^2\Big] \leq 2\E\big[g'(\mathbf X)^2\big] + \E\big[g(\mathbf X)^2\bar\eta_n(\mathbf X)^2\big] = 2\,\mathrm{I} + \mathrm{II}.
\end{gather*}
Using \autoref{thm:approx_error}, we know that $\bar\eta_n$ is bounded. Then there exists $C$ such that $\bar\eta_n\leq C$ $\mu$-a.s. for all $n$. Then, we have that
\begin{equation*}
    \mathrm{II} \leq C^2\E\big[g(\mathbf X)^2\big] \leq C^2C_2^*\|g\|_2^2
\end{equation*}
since $f_0$ is bounded away from infinity.

To conclude, we would only need to prove that
\begin{equation*}
    \|g'\|_2^2 \leq M\|g\|_2^2
\end{equation*}
for some constant $M>0$ that depends only on $[a,b]$ and $m$. Unfortunately, this is not possible. We construct a counterexample.

For simplicity, take $[a,b]=[0,1]$ and fix the spline degree $m = 2$. We consider the following sequence of knots vectors. For each $n\in\mathbb N$, we take the knots partition $t_{(n)} \in\R^{n+2}$ with
\begin{equation*}
    0 = (t_{(n)})_0 < (t_{(n)})_1 < \dots < (t_{(n)})_{n+1} = 1, \qquad (t_{(n)})_i := \frac{i}{n+1},
\end{equation*}
Let $(\mathbb G_n)_{n\in\N}$ the sequence of spline spaces. We consider the spline $g_n\in \mathbb G_n$ defined as
\begin{equation*}
    g_n(x) = \begin{cases}
    \sqrt{5(n+1)^5}\Big(x-\frac{1}{n+1}\Big)^2\qquad\ \, \text{if }0\leq x < \frac{1}{n+1} \\
    0\qquad\qquad\qquad\qquad\qquad\qquad\ \text{if }\frac{1}{n+1}\leq x \leq 1
    \end{cases}
\end{equation*}
so that $\|g_n\|_2 = 1$ for all $n\in\N$. The derivative of $g_n$ then is
\begin{equation*}
    g_n'(x) = \begin{cases}
    2\sqrt{5(n+1)^5}\Big(x-\frac{1}{n+1}\Big)\qquad\ \, \text{if }0\leq x < \frac{1}{n+1} \\
    0\qquad\qquad\qquad\qquad\qquad\qquad\ \text{if }\frac{1}{n+1}\leq x \leq 1
    \end{cases},
\end{equation*}
so that we have $\|g'\|_2\rightarrow\infty$ as $n\rightarrow\infty$. Then, there cannot exist $M>0$ such that $\|g_n'\|_2^2 \leq M\|g_n\|_2^2 = M$ for all $n\in\N$.

However, thanks to \autoref{thm:approx_error}, we know that $f_0$ can be approximated arbitrarily well in spline spaces as the number of knots increases. Moreover, for any $\psi\in L_2[a,b]$, the law of large numbers gives that
\begin{equation*}
    \ell(\psi; \mathbf X_1 , \ldots, \mathbf X_n) \xrightarrow[n\to\infty]{\text{a.s.}} \Lambda(\psi)
\end{equation*}
Therefore, although we have not been able to verify the validity of \autoref{thm:est_error}, we expect the estimator to still perform well in practice. In general, however, we don't have guarantees on how quickly $\ell(\psi)$ converges to $\Lambda(\psi)$, as finer spline spaces may lead the variance of $\ell(\psi)$ to increase.

Finally, the estimator can be made asymptotically unbiased by following the exact same process as for the Shifted Maximum Likelihood method.