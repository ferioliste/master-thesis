\subsection{Shifted Score Matching}
In this section, we present a modified version of the simple score matching estimator of the density. This will allow us to satisfy the hypothesis of \autoref{thm:approx_error}, at the cost of having a (slightly) biased estimator.

As for the simple score matching estimator, we assume $f_0(a) = f_0(b) = 0$. In addition, we assume that $f_0$ is bounded away from infinity. This means that there exists $C_2^*$ such that
\begin{equation}
    f_0\leq C_2^*\ \ \mu\text{-a.e.}\ .
\end{equation}

Similarly to Shifted Maximum Likelihood, for $0<\gamma<1$, we consider the random variable $\widetilde{\mathbf X}$ with density
\begin{equation*}
    \tilde f_0(\cdot) = (1-\gamma)f_0(\cdot) + \frac{\gamma}{b-a}.
\end{equation*}
Equivalently, $\widetilde{\mathbf X}$ is distributed as
\begin{equation*}
    \widetilde{\mathbf X} \sim 
    \begin{cases}
        \mathbf X, & \text{with probability } 1-\gamma,\\[0.2em]
        \mathbf U, & \text{with probability } \gamma,
    \end{cases}
\end{equation*}
where $\mathbf X$ has density $f_0$ on $[a,b]$ and $\mathbf U$ is independent of $\mathbf X$ and uniformly distributed on $[a,b]$. We will construct an estimator of $\tilde f_0$.

We define the score $\widetilde\psi_0$ of $\tilde f_0$ as
\begin{equation*}
    \widetilde\psi_0(x) = \frac{\de}{\de x}\ln \tilde f_0(x) = \frac{\de}{\de x}\ln\Big((1-\gamma)f_0(\cdot) + \frac{\gamma}{b-a}\Big), \qquad\text{with }x\in[a,b],
\end{equation*}
where $\widetilde\psi_0\in W^p$.

As in the simple score matching method, we want to estimate $\widetilde\psi_0$ and then use \autoref{eq:psi_to_f} to obtain an estimate of $\tilde f_0$.

Let $\psi\in W^p$. We want to minimize the expected squared distance between $\psi(\widetilde{\mathbf X})$ and $\psi_0(\widetilde{\mathbf X})$, that is
\begin{gather*}
    J(\psi) = \frac{1}{2} \E \Big[(\psi(\widetilde{\mathbf X}) - \widetilde\psi_0(\widetilde{\mathbf X}))^2\Big] \\
    = \frac{1}{2}\int_a^b \psi(x)^2 \tilde f_0(x)\de x - \int_a^b \psi(x)\widetilde\psi_0(x)\tilde f_0(x)\de x + \frac{1}{2}\int_a^b \widetilde\psi_0(x)^2\tilde f_0(x)\de x.
\end{gather*}
As in the simple score matching method, we can remove the third term as it does not depend on $\psi$. The second term can be rewritten as
\begin{gather*}
    \int_a^b \psi(x)\widetilde\psi_0(x)\tilde f_0(x)\de x = \int_a^b \psi(x)\tilde f_0'(x)\de x = (1-\gamma)\int_a^b \psi(x)f_0'(x)\de x = \\
    (1-\gamma)\big[\psi(x)f_0(x)\big]_a^b - (1-\gamma)\int_a^b \psi'(x)f_0(x)\de x = - (1-\gamma)\int_a^b \psi'(x)f_0(x)\de x
\end{gather*}
where the first equality uses the fact that
\begin{equation*}
    \tilde f_0(x)\widetilde\psi_0(x) = \tilde f_0(x)\frac{\de}{\de x}\ln \tilde f_0(x) = \tilde f_0(x)\frac{\tilde f_0'(x)}{\tilde f_0(x)} = \tilde f_0'(x),
\end{equation*}
the second equality is by integration by parts, and the third equality uses the fact that $f_0(a) = f_0(b) = 0$. The first term can be rewritten as
\begin{gather*}
    \frac{1}{2}\int_a^b \psi(x)^2 \tilde f_0(x)\de x = \frac{1}{2}\int_a^b \psi(x)^2 \bigg((1-\gamma)f_0(x) + \frac{\gamma}{b-a}\bigg)\de x \\
    = \frac{1}{2}(1-\gamma)\int_a^b \psi(x)^2 f_0(x)\de x + \frac{1}{2} \frac{\gamma}{b-a}\int_a^b \psi(x)^2 \de x.
\end{gather*}

As before, we multiply $J(\psi)$ by $-1$ in order to turn a loss function into a utility function.

Then, we define $\Lambda$ as
\begin{gather}
    \Lambda(\psi) = - (1-\gamma)\int_a^b \psi'(x)f_0(x)\de x -\frac{1}{2}(1-\gamma)\int_a^b \psi(x)^2 f_0(x)\de x - \frac{1}{2}\frac{\gamma}{b-a}\int_a^b \psi(x)^2 \de x \notag \\
    = \E\bigg[\!- (1-\gamma)\psi'(\mathbf X) -\frac{1}{2}(1-\gamma)\psi(\mathbf X)^2\bigg] - \frac{1}{2}\frac{\gamma}{b-a}\int_a^b \psi(x)^2 \de x, \label{eq:Lambda_shifted_SM}
\end{gather}
so that $\ell$ can be defined as
\begin{gather}
    \ell(\psi; \mathbf X_1 , \ldots, \mathbf X_n) = \frac{1}{n}\sum_{i=1}^n l(\psi; \mathbf X_i) \notag \\
    = \frac{1}{n}\sum_{i=1}^n \bigg(\!- (1-\gamma)\psi'(\mathbf X_i) - \frac{1}{2}(1-\gamma)\psi(\mathbf X_i)^2 - \frac{1}{2}\frac{\gamma}{b-a}\int_a^b \psi(x)^2 \de x\bigg). \label{eq:ell_shifted_SM}
\end{gather}

We notice that, in practice, the integral $\int_a^b \psi(x)^2\de x$ does not need to be approximated numerically. Indeed, for $\psi\in\mathbb G$, $\psi^2$ is a polynomial on each interval $[t_i,t_{i+1}]$. Therefore, the integral can be computed exactly as
\begin{equation*}
    \int_a^b \psi(x)^2\de x = \sum_{i=1}^k \int_{t_i}^{t_{i+1}} \psi(x)^2\de x.
\end{equation*}

First we show that $\ell$ is a valid utility function. Then, we verify that $\Lambda$ satisfies the hypothesis of \autoref{lem:cond_1}, so that we are able to apply \autoref{thm:approx_error}.

\begin{proposition}
    The function $\ell$ as in \autoref{eq:ell_shifted_SM} is a valid utility function.
\end{proposition}

\begin{proof}\mbox{}

We verify the hypotheses of \autoref{prop:validity} so we can conclude that $\ell$ is valid.

Condition (i) is satisfied by construction.

We define $\tilde\ell$ and $\tilde\Lambda$ as in \autoref{prop:validity}. We have
\begin{gather*}
    \tilde\ell(\theta; \mathbf X_1 , \ldots, \mathbf X_n) \\
    = \frac{1}{n}\sum_{v=1}^n \bigg(\!- (1-\gamma)\theta^\tr \varphi'(\mathbf X_v) - \frac{1}{2}(1-\gamma)\big(\theta^\tr \varphi(\mathbf X_v)\big)^2 - \frac{1}{2}\frac{\gamma}{b-a}\int_a^b \psi(x)^2 \de x\bigg)
\end{gather*}
and
\begin{equation*}
    \Lambda(\theta) = \E\bigg[\!- (1-\gamma)\theta^\tr \varphi'(\mathbf X) - \frac{1}{2}(1-\gamma)\big(\theta^\tr \varphi(\mathbf X)\big)^2\bigg] - \frac{1}{2}\frac{\gamma}{b-a}\int_a^b \psi(x)^2 \de x,
\end{equation*}
for $\theta\in\R^{m+k+1}$ and $\{\varphi_0,\ldots, \varphi_{m+k}\}$ basis of $\mathbb G$.

In order to verify condition (ii), we differentiate $\ell$ twice, we find
\begin{equation*}
    \big(\nabla^2_{\theta} \tilde\ell(\theta;\mathbf W_1,\ldots,\mathbf W_n)\big)_{i,j} = -\frac{1}{n}\sum_{v=1}^n (1-\gamma)\varphi_i(\mathbf X_v)\varphi_j(\mathbf X_v) - \frac{\gamma}{b-a}\int_a^b \varphi_i(x)\varphi_j(x)\de x
\end{equation*}
so that
\begin{equation*}
    \nabla^2_{\theta} \tilde\ell(\theta;\mathbf W_1,\ldots,\mathbf W_n) = -\frac{1}{n}\sum_{v=1}^n (1-\gamma)\varphi(\mathbf X_v)\varphi(\mathbf X_v)^\tr - \frac{\gamma}{b-a}G_0,
\end{equation*}
with $(G_0)_{i,j} = \int_a^b \varphi_i(x)\varphi_j(x)\de x$.

Take $u\in\R^{m+k+1}$. Then
\begin{gather*}
    u^\tr G_0 u = \sum_{i=0}^{m+k}\sum_{j=0}^{m+k}u_i u_j (G_0)_{i,j} = \sum_{i=0}^{m+k}\sum_{j=0}^{m+k}u_i u_j \int_a^b \varphi_i(x)\varphi_j(x)\de x \\
    = \int_a^b \bigg(\sum_{i=0}^{m+k} \theta_i\varphi_i(x)\bigg)^2 \de x \geq 0
\end{gather*}
where the equality holds if and only if $u=0$. Then, $G_0$ is strictly positive definite.

Since $\varphi(\mathbf X_v)\varphi(\mathbf X_v)^\tr$ is positive semi-definite for all $v\in{1,\ldots,n}$ and $G_0$ is strictly positive definite, $\nabla^2_{\theta} \tilde\ell(\theta;\mathbf W_1,\ldots,\mathbf W_n)$ is strictly negative definite for all $\theta\in\R^{m+k+1}$.

In order to verify condition (iii), we differentiate $\Lambda$ twice, we find
\begin{equation*}
    \big(\nabla^2_{\theta} \tilde\Lambda(\theta)\big)_{i,j} = (1-\gamma)\E\Big[\varphi_i(\mathbf X)\varphi_j(\mathbf X)\Big] - \frac{\gamma}{b-a}\int_a^b \varphi_i(x)\varphi_j(x)\de x,
\end{equation*}
so that
\begin{equation*}
    \nabla^2_{\theta} \tilde\Lambda(\theta) = (1-\gamma)\E\big[\varphi(\mathbf X)\varphi(\mathbf X)^\tr\big] - \frac{\gamma}{b-a}G_0.
\end{equation*}

Since $\E\big[\varphi(\mathbf X)\varphi(\mathbf X)^\tr\big]$ is positive semi-definite and $G_0$ is strictly positive definite, $\nabla^2_{\theta} \tilde\Lambda(\theta)$ is strictly negative definite for all $\theta\in\R^{m+k+1}$.
\end{proof}

\begin{proposition}
The functional $\Lambda$ as in \autoref{eq:Lambda_shifted_SM} satisfies the hypothesis of \autoref{lem:cond_1}, and so condition (1) of \autoref{thm:approx_error} is satisfied.
\end{proposition}

\begin{proof}\mbox{}

We have that
\begin{gather*}
    \Lambda(h_1 + \alpha h_2) = \E\bigg[\!- (1-\gamma)\big(h_1'(\mathbf X) + \alpha h_2'(\mathbf X)\big) -\frac{1}{2}(1-\gamma)\big(h_1(\mathbf X)+ \alpha h_2(\mathbf X)\big)^2\bigg] \\
    - \frac{1}{2}\frac{\gamma}{b-a}\int_a^b \big(h_1(x)+ \alpha h_2(x)\big)^2 \de x\qquad\text{and} \\
    \frac{\de^2}{\de\alpha^2}\Lambda(h_1 + \alpha h_2)
    = \E\big[\!-(1-\gamma)h_2(\mathbf X)^2\big]
    - \frac{\gamma}{b-a}\int_a^b h_2(x)^2 \de x.
\end{gather*}

Since $f_0$ is bounded away from infinity,
\begin{equation*}
    0\leq\E\big[h_2(\mathbf X)^2\big]\leq C_2^*\|h_2\|_2^2.
\end{equation*}
Then,
\begin{equation*}
    -\Big((1-\gamma)C_2^* + \frac{\gamma}{b-a}\Big)\|h_2\|_2^2 \leq \Lambda(h_1 + \alpha h_2) \leq -\frac{\gamma}{b-a}\|h_2\|_2^2.
\end{equation*}

Since the hypotheses are satisfied, we can use \autoref{lem:cond_1} and conclude that in this setting condition (2) of \autoref{thm:approx_error} is satisfied.
\end{proof}

Unfortunately, to our knowledge, $l$ still does not satisfy the hypothesis of \autoref{thm:est_error}. We report here our attempt to prove the hypothesis of \autoref{lem:cond_2_bis} and explain why it fails.

We recall that
\begin{equation*}
    \dot l[\bar\eta_n; g] = \frac{\de}{\de\alpha}l(\bar\eta_n+\alpha g)\bigg|_{\alpha=0^+}.
\end{equation*}

For $h\in\mathbb G$ with $\|h\|_2\leq 1$, we have
\begin{gather*}
    l(\bar\eta_n+\alpha g) = - (1-\gamma)\big(\bar\eta_n'(\mathbf X)+\alpha g'(\mathbf X)\big) - \frac{1}{2}(1-\gamma)\big(\bar\eta_n(\mathbf X)+\alpha g(\mathbf X)\big)^2 \\
    - \frac{1}{2}\frac{\gamma}{b-a}\int_a^b \big(\bar\eta_n(x)+\alpha g(x)\big)^2 \de x,\\
    \frac{\de}{\de\alpha}l(\bar\eta_n+\alpha g) = - (1-\gamma)g'(\mathbf X) - \frac{1}{2}(1-\gamma)g(\mathbf X)\big(\bar\eta_n(\mathbf X)+\alpha g(\mathbf X)\big) \\
    - \frac{1}{2}\frac{\gamma}{b-a}\int_a^b g(x)\big(\bar\eta_n(x)+\alpha g(x)\big) \de x, \text{ and}\\
    \dot l[\bar\eta_n; g] = - (1-\gamma)g'(\mathbf X) - \frac{1}{2}(1-\gamma)g(\mathbf X)\bar\eta_n(\mathbf X) - \frac{1}{2}\frac{\gamma}{b-a}\int_a^b g(x)\bar\eta_n(x)\de x.
\end{gather*}
It follows that
\begin{gather*}
    \var\big(\dot l[\bar\eta_n; g]\big) \leq \E\Big[\Big(- (1-\gamma)g'(\mathbf X) - \frac{1}{2}(1-\gamma)g(\mathbf X)\bar\eta_n(\mathbf X)\Big)^2\Big] \\
    \leq \E\Big[\Big(g'(\mathbf X) + \frac{1}{2}g(\mathbf X)\bar\eta_n(\mathbf X)\Big)^2\Big] \leq 2\E\big[g'(\mathbf X)^2\big] + \E\big[g(\mathbf X)^2\bar\eta_n(\mathbf X)^2\big] = 2\,\mathrm{I} + \mathrm{II}.
\end{gather*}
Using \autoref{thm:approx_error}, we know that $\bar\eta_n$ is bounded. Then there exists $C$ such that $\bar\eta_n\leq C$ $\mu$-a.s. for all $n$. Then, we have that
\begin{equation*}
    \mathrm{II} \leq C^2\E\big[g(\mathbf X)^2\big] \leq C^2C_2^*\|g\|_2^2
\end{equation*}
since $f_0$ is bounded away from infinity.

To conclude, we would only need to prove that
\begin{equation*}
    \|g'\|_2^2 \leq M\|g\|_2^2
\end{equation*}
for some constant $M>0$ that depends only on $[a,b]$ and $m$. Unfortunately, this is not possible. We construct a counterexample.

For simplicity, take $[a,b]=[0,1]$ and fix the spline degree $m = 2$. We consider the following sequence of knots vectors. For each $n\in\mathbb N$, we take the knots partition $t_{(n)} \in\R^{n+2}$ with
\begin{equation*}
    0 = (t_{(n)})_0 < (t_{(n)})_1 < \dots < (t_{(n)})_{n+1} = 1, \qquad (t_{(n)})_i := \frac{i}{n+1},\text{ for }i=0,\ldots,n+1.
\end{equation*}
Let $(\mathbb G_n)_{n\in\N}$ the sequence of spline spaces. We consider the spline $g_n\in \mathbb G_n$ defined as
\begin{equation*}
    g_n(x) = \begin{cases}
    \sqrt{5(n+1)^5}\Big(x-\frac{1}{n+1}\Big)^2\qquad\ \, \text{if }0\leq x < \frac{1}{n+1} \\
    0\qquad\qquad\qquad\qquad\qquad\qquad\ \text{if }\frac{1}{n+1}\leq x \leq 1
    \end{cases}
\end{equation*}
so that $\|g_n\|_2 = 1$ for all $n\in\N$. The derivative of $g_n$ then is
\begin{equation*}
    g_n'(x) = \begin{cases}
    2\sqrt{5(n+1)^5}\Big(x-\frac{1}{n+1}\Big)\qquad\ \, \text{if }0\leq x < \frac{1}{n+1} \\
    0\qquad\qquad\qquad\qquad\qquad\qquad\ \text{if }\frac{1}{n+1}\leq x \leq 1
    \end{cases},
\end{equation*}
so that we have $\|g'\|_2\rightarrow\infty$ as $n\rightarrow\infty$. Then, there cannot exist $M>0$ such that $\|g_n'\|_2^2 \leq M\|g_n\|_2^2 = M$ for all $n\in\N$.

However, thanks to \autoref{thm:approx_error}, we know that $f_0$ can be approximated arbitrarily well in spline spaces as the number of knots increases. Moreover, for any $\psi\in L_2[a,b]$, the law of large numbers gives that
\begin{equation*}
    \ell(\psi; \mathbf X_1 , \ldots, \mathbf X_n) \xrightarrow[n\to\infty]{\text{a.s.}} \Lambda(\psi)
\end{equation*}
Therefore, although we have not been able to verify the validity of \autoref{thm:est_error}, we expect the estimator to still perform well in practice. In general, however, we don't have guarantees on how quickly $\ell(\psi)$ converges to $\Lambda(\psi)$, as finer spline spaces may lead the variance of $\ell(\psi)$ to increase.

Finally, the estimator can be made asymptotically unbiased by following the exact same process as for the Shifted Maximum Likelihood method.