\subsection{Optimization algorithm}
We assume to have available $n$ iid realizations of $\mathbf X$, denoted as $\mathbf X_1,\dots,\mathbf X_n$.

We recall that all the estimators that we presented are in the form
\begin{equation*}
    \hat\eta_n = \argmax_{h\in \mathbb G} \p\ell(h; \mathbf X_1 , \ldots, \mathbf X_n).
\end{equation*}

Using the representation \ref{eq:representation}, we can reformulate the problem as
\begin{equation*}
    \hat \theta = \argmax_{\theta\in\R^{m+k+1}} \mathcal L(\theta)\qquad\text{with}\qquad\mathcal L(\theta) = \p\ell\Big(\theta^\tr \varphi(x); \mathbf X_1 , \ldots, \mathbf X_n\Big)
\end{equation*}
and $\hat\eta_n = \hat\theta^\tr \varphi(x)$.

In order to maximize $\mathcal L$ we use the Newton-Raphson method. We start with an initial guess $\theta^{(0)}$ and we produce a sequence of coefficient vectors using the formula
\begin{equation*}
    \theta^{(i+1)} = \theta^{(i)} + \zeta\big[\H(\theta^{(i)})\big]^{-1} \S(\theta^{(i)})
\end{equation*}
where $0<\zeta\leq 1$, $\S(\theta^{(i)}) = \nabla\mathcal L(\theta^{(i)}) \in\R^{m+k+1}$ is the gradient of $\mathcal L$ at $\theta^{(i)}$ and $\H(\theta^{(i)}) = \nabla^2\mathcal L(\theta^{(i)}) \in\R^{(m+k+1)\times (m+k+1)}$ is the Hessian of $\mathcal L$ at $\theta^{(i)}$. The idea behind the Newton-Raphson method is to use a second-order Taylor expansion of $\mathcal L$ at $\theta^{(i)}$ and to move towards the stationary point of that quadratic approximation.

We stop the iterations when
\begin{equation*}
    \frac{\mathcal L(\theta^{(i+1)}) - \mathcal L(\theta^{(i)})}{\max\{|\mathcal L(\theta^{(i)}|,\, 1\}} \leq \varepsilon
\end{equation*}
for $\varepsilon > 0$.

Finally, we state a simple sufficient condition that guaranties that any limit point of the algorithm is a global maximizer. 

\begin{proposition}\label{prop:newton_global_max}
    Let $\mathcal L:\R^{m+k+1}\to\R$ and consider the maximization problem
    \begin{equation*}
        \max_{\theta\in\R^{m+k+1}} \mathcal L(\theta).
    \end{equation*}
    Let $(\theta^{(i)})_{i\in\N}$ be the sequence generated by the Newton-Raphson algorithm applied to $\mathcal L$. Assume that:
    \begin{enumerate}
        \item $\mathcal L$ is twice continuously differentiable on $\R^{m+k+1}$;
        \item $\H(\theta)=\nabla^2\mathcal L(\theta)$ is negative semi-definite for all $\theta\in\R^{m+k+1}$, so that $\mathcal L$ is concave;
        \item $(\theta^{(i)})_{i\in\N}$ converges to some $\hat\theta\in\R^{m+k+1}$.
    \end{enumerate}
    Then $\hat\theta$ is a global maximizer of $\mathcal L$, that is,
    \begin{equation*}
        \mathcal L(\hat\theta)=\max_{\theta\in\R^{m+k+1}} \mathcal L(\theta).
    \end{equation*}
\end{proposition}

\begin{algorithm}[H]
\caption{General algorithm for the maximization of $\mathcal L$}
\label{alg:newton_max}
\begin{algorithmic}[1]
\REQUIRE Data $\mathbf X_1,\ldots,\mathbf X_n$, initial value $\theta^{(0)}\in\R^{m+k+1}$, step size $\zeta\in(0,1]$, tolerance $\varepsilon>0$, maximum number of iterations $I_{\max}$.
\STATE Compute $G_q$
\STATE Set $i\gets 0$.
\STATE Set $r_0 = 1 + \varepsilon$
\WHILE{$r_i \geq \varepsilon$ \AND $i < I_{\max}$}
    \STATE Compute the gradient $\S(\theta^{(i)}) = \nabla\mathcal L(\theta^{(i)})$.
    \STATE Compute the Hessian $\H(\theta^{(i)}) = \nabla^2\mathcal L(\theta^{(i)})$.
    \STATE Solve the linear system $\H(\theta^{(i)})\,d^{(i)} = \S(\theta^{(i)})$ for $d^{(i)}\in\R^{m+k+1}$.
    \STATE Set $\theta^{(i+1)} \gets \theta^{(i)} + \zeta\, d^{(i)}$.
    \STATE Set $i\gets i+1$.
    \STATE Set $\displaystyle r_i \gets \frac{\mathcal L(\theta^{(i+1)}) - \mathcal L(\theta^{(i)})}{\max\{|\mathcal L(\theta^{(i)})|,\, 1\}}.$
\ENDWHILE
\RETURN $\theta^{(i)}$
\end{algorithmic}
\end{algorithm}