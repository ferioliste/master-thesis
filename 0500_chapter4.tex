\section{Numerical Experiments}
In this last chapter, we present the results of the numerical experiments we conducted to compare the five methods for probability density estimation.

We first study the methods on synthetic data, where the ground-truth density is known, and then we consider two real-world applications.

We introduce two unnormalized densities $f_1$ and $f_2$, defined on $[a,b]=[-2,2]$ by
\begin{equation*}
    f_1(x) = e^{-(x-2)^4} + \tfrac{1}{2}e^{-(x+2)^4} + 0.05
\end{equation*}
and
\begin{equation*}
    f_2(x) =
    \Big(
        e^{-\sqrt{16(x - 0.5)^2 + 1}}
        + 2\,e^{-\sqrt{16(x + 0.5)^2 + 1}}
        + 4\,e^{-\sqrt{128(x - 1.25)^2 + 1}}
    \Big)(4 - x^2).
\end{equation*}
We choose $f_1$ and $f_2$ on purpose to be non-polynomial $C^\infty$ functions. This is important since otherwise estimating them would be a trivial task and a not so useful benchmark for the considered methods.

\begin{figure}[hbt]
\centering
\includegraphics[width=.8\linewidth]{plots/distributions.pdf}
\caption{Normalized target densities $f_1$ and $f_2$ on $[a,b]=[-2,2]$, used in the synthetic experiments.}
\label{fig:distributions}
\end{figure}

We have that $f_1$ is bounded away from $0$ and infinity, while $f_2$ satisfies the boundary condition $f_2(a)=f_2(b)=0$. These properties determine which estimation algorithms can be applied. In particular, $f_1$ can be used with the Simple Maximum Likelihood, Shifted Maximum Likelihood, and Generalized Score Matching methods, while $f_2$ can be used with the Shifted Maximum Likelihood, Simple Score Matching, Shifted Score Matching, and Generalized Score Matching methods.

In order to generate samples from $f_1$ and $f_2$, we use the acceptance-rejection method, which only requires knowledge of the unnormalized density. This allows us to compare the estimated densities with the normalized ground-truth.

Throughout this chapter, we fix the spline degree to $m=3$, as in \cite{kooperberg1992}, and we use the penalty order $q=2$. In our experiments, provided that $1/2<q\leq m$, we did not observe substantial differences in the qualitative behavior of the estimators when varying $(m,q)$. For this reason, we keep these choices fixed in all the numerical results reported below.

In all experiments we use uniformly spaced knots on $[a,b]$. For a given number of interior knots $k$, we take $t_i = a+i(b-a)/(k+1)$ for $i=0,\ldots,k+1$. 

In addition, we use the following fixed parameters:
\begin{itemize}
    \item We initialize the optimizations at $\theta^{(0)}=0$.
    \item We use Gauss-Legendre quadrature with $\alpha=6$ nodes.
    \item In the Newton-Raphson method use a fixed step size $\zeta=0.1$, we perform at most $I_{\max}=1000$ iterations, and we stop the iterations when $r$ falls below $10^{-6}$
    \item For Generalized Score Matching, we set $\beta=2$, as in \cite{gen_sm}.
\end{itemize}

\input{0501_n_vs_k}
\input{0502_alg_vs_n}
\input{0503_lambda_n}
\input{0504_real_test_cases}
\input{0505_runtime}